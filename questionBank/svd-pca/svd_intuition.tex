% Author: Taejin Hwang

\qns{SVD Intuition}

Now that we've taken a look at symmetric matrices, specifically $A^{T} A,$ we will investigate into some of the intuition behind the "Swiss-Army Knife" of Linear Algebra, the \textbf{Singular Value Decomposition.} You may have seen the following forms of the SVD of an $m \times n$ matrix $A$ from lecture, homework, and discussion:
\begin{equation}
  A = U \Sigma V^{T} = \sum\limits_{i} \sigma_{i} \vec{u}_{i} \vec{v}_{i}^T
\end{equation}
Where $\sigma_{i}$ are referred to as the \textbf{singular values} and $\vec{u}_{i}, \vec{v}_{i}$ are the $i^{th}$ column of $U$ and $V$ respectively.

The Spectral Theorem which says that any \textbf{symmetric} matrix is orthogonally diagonalizable, will play a huge part in this question. 
This means that a symmetric matrix will always have $n$ linearly independent eigenvectors that are all mutually orthogonal.

We will start our investigation by looking at the eigenvectors of the matrix $A^{T} A.$

\begin{enumerate}
  \qitem Let $\vec{v}$ be a nonzero vector in $\mathbb{R}^n$ that is not in the $\text{Nul}(A).$ 
  Show that if $\vec{v}$ is an eigenvector of $A^{T} A,$ with eigenvalue $\lambda,$ there must also be an eigenvector $\vec{u}$ of $AA^{T}$ with eigenvalue $\lambda.$

  \sol {
    Since $\vec{v}$ is an eigenvector of $A^{T} A,$ we know that $A^{T}A \vec{v} = \lambda \vec{v}.$ \vskip 1pt
    If we left multiply by $A$ we see that 
    \begin{equation}
      A A^{T}A \vec{v} = A (\lambda \vec{v}) = \lambda (A \vec{v}) 
    \end{equation}
    We assumed that $\vec{v}$ is not in the $\text{Nul}(A).$ This means that $A \vec{v}$ will be nonzero, and will in fact be in the $\text{Col}(A).$
    Therefore, we can say that $\vec{u} = A \vec{v}$ is an eigenvector of $AA^{T}$ with eigenvalue $\lambda.$
  }

  \qitem Let $\vec{u}$ be a nonzero vector in $\mathbb{R}^m$ that is not in the $\text{Nul}(A^{T}).$ 
  Show that if $\vec{u}$ is an eigenvector of $AA^{T},$ with eigenvalue $\lambda,$ there must also be an eigenvector $\vec{v}$ of $A^{T} A$ with eigenvalue $\lambda.$

  \sol {
    We proceed in a similar manner: Since $\vec{v}$ is an eigenvector of $AA^{T},$ we know that $AA^{T} \vec{u} = \lambda \vec{u}.$ \vskip 1pt
    If we left multiply by $A^{T}$ we see that 
    \begin{equation}
      A^{T}AA^{T} \vec{u} = A^{T} (\lambda \vec{u}) = \lambda (A^{T} \vec{u})
    \end{equation}
    We again assumed $\vec{u}$ is not in the $\text{Nul}(A^{T})$ so $A^{T} \vec{u}$ will be nonzero and in the $\text{Col}(A^{T}).$
    Therefore, we can say that $\vec{v} = A^{T} \vec{u}$ is an eigenvector of $A^{T}A$ with eigenvalue $\lambda.$
  }

  \qitem We can pick $\vec{v}$ to be an eigenvector of $A^{T}A$ with unit norm. What would the norm of $A \vec{v},$ be?

  \sol {
    The $l^{2}$ norm of a vector is the square root of its inner product with itself:
    \begin{equation} 
      \norm{A \vec{v}}_{2} = \sqrt{(A \vec{v})^{T} (A \vec{v})} = \sqrt{\vec{v}^{T} A^{T} A \vec{v}} = \sqrt{\vec{v}^{T} \lambda \vec{v}} = \sqrt{\lambda \vec{v}^{T} \vec{v}} = \sqrt{\lambda \norm{v}_2} = \sqrt{\lambda}
    \end{equation}
    Note that $\norm{A \vec{v}}_{2}$ cannot be imaginary since we showed that $\lambda \geq 0$ in the previous question in part (b).
  }

  \qitem Going back to part (a), let's normalize $\vec{u}$ and $\vec{v}$ to have unit norm.
  Can you give a relationship between $\vec{u}$ and $\vec{v}$ using the fact that they have unit norm?

  \sol {
    We showed in part (c) that if $\vec{v}$ had unit norm, $\norm{A \vec{v}} = \sqrt{\lambda}.$ \vskip 1pt
    In part (a) we showed that $A \vec{v}$ is an eigenvector of $A A^{T}.$ 
    Therefore, we can say that $A \vec{v} = \sigma \vec{u}$ for some scalar $\sigma.$ 
    Taking the norm of both sides, we will see that:
    \begin{equation}
      \norm{A \vec{v}} = \norm{\sigma \vec{u}} = \sigma \norm{\vec{u}} = \sigma
    \end{equation}
    This shows that $\sigma = \sqrt{\lambda}$ so we see that $A \vec{v} = \sqrt{\lambda} \vec{u}.$
  }

  \qitem Now let $\vec{v}$ be nonzero vector in $\mathbb{R}^{n}$ that is also in the null space of $A.$
  Show that $\vec{v}$ is an eigenvector of $A^{T} A$ with eigenvalue $0.$

  \sol {
    If $\vec{v} \in \text{Nul}(A),$ then $A \vec{v} = \vec{0}.$ Left multiplying, by $A^{T}$ we see that:
    \begin{equation}
      A^{T} (A \vec{v}) = A^{T} \vec{0} = \vec{0} = 0 \cdot \vec{v}
    \end{equation}
    Therefore, $\vec{v}$ is an eigenvector of $A^{T} A$ with eigenvalue $0.$
  }

  \qitem Similarly, let $\vec{u}$ be nonzero vector in $\mathbb{R}^{m}$ that is also in the null space of $A^{T}.$
  Show that $\vec{u}$ is an eigenvector of $AA^{T}$ with eigenvalue $0.$

  \sol {
    If $\vec{u} \in \text{Nul}(A^{T}),$ then $A^{T} \vec{u} = \vec{0}.$ Left multiplying, by $A$ we see that:
    \begin{equation}
      A (A^{T} \vec{u}) = A \vec{0} = \vec{0} = 0 \cdot \vec{u}
    \end{equation}
    Therefore, $\vec{u}$ is an eigenvector of $A A^{T}$ with eigenvalue $0.$
  }

  \qitem We will finally bring everything together using all of the facts proven above.
  If $A$ is a $m \times n$ matrix, show that $AV = U \Sigma.$ \textit{Hint: Try writing out the relationship between $\vec{v}_{i}$ and $\vec{u}_{i}$ and try turning these vector equations into a matrix equation.}

  \sol {
    We will consider the cases for when $m \geq n$ and $m < n.$ \\
    Suppose $m \geq n.$ \\
    We've shown for eigenvectors of $A^{T} A, \ \vec{v}_{1}, \cdots, \vec{v_k} \in \text{Col}(A), \ A \vec{v}_{1} = \sigma_{1} \vec{u}_{1}, \cdots, A \vec{v_k} = \vec{u_k}.$ \\
    Therefore, if we try turning this into a matrix equation, we see that:
    $$A \begin{bmatrix} | & \ & | \\
    \vec{v}_{1} & \cdots & \vec{v}_{k}  \\ | & \ & | \end{bmatrix} 
    =  \begin{bmatrix} | & \ & | \\ \sigma_{1} \vec{u}_{1} & \cdots & \sigma_{k} \vec{u}_{k} \\ | & \ & | \end{bmatrix}$$
    For the remaining eigenvectors of $A^{T}A, \vec{v}_{k + 1}, \cdots, \vec{v}_{n} \in \text{Nul}(A), \ A \vec{v_{i}} = 0 \vec{u_{i}},$ up until $n.$ Therefore, we can try extending our matrix to all of the columns of $V.$\\
    $$A \begin{bmatrix} | & \ & | \\
    \vec{v}_{1} & \cdots & \vec{v}_{n}  \\ | & \ & | \end{bmatrix} 
    =  \begin{bmatrix} | & \ & | \\ \sigma_{1} \vec{u}_{1} & \cdots & \sigma_{k} \vec{u}_{n} \\ | & \ & | \end{bmatrix} = 
     \begin{bmatrix} | & \ & | \\ \vec{u}_{1} & \cdots & \vec{u}_{n} \\ | & \ & | \end{bmatrix} \begin{bmatrix} \sigma_{1} & 0 &  \cdots & 0 \\ 0 & \sigma_{2} & \cdots & 0 \\ \vdots & \vdots & \ddots & \sigma_{n} \\ 
    \vdots & \vdots & \ddots & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix} = U \Sigma$$
    Now let's suppose $m < n.$ \\
    We start off with the eigenvectors of $A^{T} A$ again: $A \vec{v}_{1} = \sigma_{1} \vec{u}_{1}, \cdots, A \vec{v_k} = \vec{u_k}.$ \\
    We will get the same matrix equation:
    $$A \begin{bmatrix} | & \ & | \\
    \vec{v}_{1} & \cdots & \vec{v}_{k}  \\ | & \ & | \end{bmatrix} 
    =  \begin{bmatrix} | & \ & | \\ \sigma_{1} \vec{u}_{1} & \cdots & \sigma_{k} \vec{u}_{k} \\ | & \ & | \end{bmatrix}$$
    For the remaining eigenvectors of $A^{T}A, \vec{v}_{k + 1}, \cdots, \vec{v}_{n} \in \text{Nul}(A), \ A \vec{v_{i}} = 0 \vec{u_{i}},$ up until $m < n.$ Notice that we cannot cover all eigenvectors of $A^{T} A$ but we will cover up to $m < n.$
    $$A \begin{bmatrix} | & \ & | \\
    \vec{v}_{1} & \cdots & \vec{v}_{m}  \\ | & \ & | \end{bmatrix} = 
    \begin{bmatrix} | & \ & | \\ \sigma_{1} \vec{u}_{1} & \cdots & \sigma_{m} \vec{u}_{m} \\ | & \ & | \end{bmatrix} = 
     \begin{bmatrix} | & \ & | \\ \vec{u}_{1} & \cdots & \vec{u}_{m} \\ | & \ & | \end{bmatrix} \begin{bmatrix} \sigma_{1} & 0 &  \cdots & 0 \\ 0 & \sigma_{2} & \cdots & 0 \\ \vdots & \vdots & \ddots & \sigma_{m} \end{bmatrix}$$
    We however, can still extend to $V$ since $A \vec{v}_{i} = \vec{0}$ for $i \geq n.$ We will do this by padding $0$'s to the right of the $\Sigma$ matrix.
    $$A \begin{bmatrix} | & \ & | \\
    \vec{v}_{1} & \cdots & \vec{v}_{n}  \\ | & \ & | \end{bmatrix} = 
    \begin{bmatrix} | & \ & | &  & | \\ \sigma_{1} \vec{u}_{1} & \cdots & \sigma_{m} \vec{u}_{m} & \cdots & \vec{0} \\ | & \ & | &  & | \end{bmatrix} = 
     \begin{bmatrix} | & \ & | \\ \vec{u}_{1} & \cdots & \vec{u}_{m} \\ | & \ & | \end{bmatrix} \begin{bmatrix} \sigma_{1} & 0 & \cdots & 0 & \cdots & 0 \\ 0 & \sigma_{2} & \cdots & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \sigma_{m} & \cdots&  0 \end{bmatrix}$$
    In both cases, we see that $AV = U\Sigma .$
  }

  \qitem Why does the above equation imply that $A = U \Sigma V^{T}?$

  \sol {
    Since $V$ is an orthonormal matrix, we can right multiply by $V^{-1} = V^{T}$ to get:
    \begin{equation}
      A = U \Sigma V^{T}
    \end{equation}
  }

  \qitem \textbf{OPTIONAL:} The Singular Value Decomposition will compute the eigenvectors of $A^{T} A$ and $AA^{T}.$ 
  Based on the previous parts, we can make the realization that each of these eigenvectors are part of one of the four fundamental subspaces of $A: \text{Col}(A), \text{Nul}(A), \text{Col}(A^{T}), \text{Nul}(A^{T}).$
  Show which eigenvectors of $A^{T} A$ and $AA^{T}$ belong to which subspaces of $\mathbb{R}^{n}$ and $\mathbb{R}^{m}.$ 

  \sol {

    The matrix $AA^{T}$ is an $m \times m$ matrix. \\
    We showed in part (a) that there are eigenvectors $\vec{u}$ of $AA^{T}$ that are in $\text{Col}(A).$ \\
    We also showed in part (f) that there are eigenvectors $\vec{u}$ of $AA^{T}$ with eigenvalue $0,$ that are in $\text{Nul}(A^{T}).$ 

    The matrix $A^{T}A$ is an $n \times n$ matrix. \\
    We showed in part (b) that there are eigenvectors $\vec{v}$ of $A^{T}A$ that are in $\text{Col}(A^{T}).$ \\
    We also showed in part (e) that there are eigenvectors $\vec{v}$ of $A^{T}A$ with eigenvalue $0,$ that are in $\text{Nul}(A).$ 
  }

  \qitem \textbf{OPTIONAL:} The Fundamental Theorem of Linear Algebra states that:
  \begin{align*}
    \text{Col}(A) &\perp \text{Nul}(A^{T}) \\
    \text{Col}(A^{T}) &\perp \text{Nul}(A)
  \end{align*}  
  Confirm that the Fundamental Theorem of Linear Algebra is true using the eigenvectors of $A^{T}A$ and $AA^{T},$ and that they are in fact basis vectors for these fundamental subspaces.

  \sol {
    We know from the Spectral Theorem that the eigenvectors $\vec{v}_{i}$ of $A^{T}A$ are all mutually orthogonal.
    This confirms that the eigenvectors in $\text{Col}(A^{T})$ will be orthogonal to those in $\text{Nul}(A).$
    We also know from the Spectral Theorem, that $A^{T}A$ has $n$ linealry independent eigenvectors, so the $\vec{v}_i$ must form a basis for $\mathbb{R}^{n}.$ 
    Every eigenvector $\vec{v}_{i}$ will either be in the $\text{Col}(A^{T})$ or $\text{Nul}(A)$ since the dimensions of these two spaces sum to $n$ by the Rank-Nullity Theorem.
    Therefore, the eigenvectors in $\text{Col}(A^{T})$ span the entirety of the space, and are linearly independent by construction. 
    Similarly, the eigenvectors in $\text{Nul}(A)$ also span the entirety of the space, and are linearly independent. 
    We conclude by saying that the eigenvectors of $A^{T}A$ in $\text{Col}(A^{T})$ are a basis for $\text{Col}(A^{T}),$ and the eigenvectors of $A^{T}A$ in $\text{Nul}(A)$ are also a basis for $\text{Nul}(A).$ \\
    The exact same argument can be made for the eigenvectors of $AA^{T}$ using the Spectral Theorem, and the Rank-Nullity Theorem.

  }

\end{enumerate}