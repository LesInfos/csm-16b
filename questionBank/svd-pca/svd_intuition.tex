% Author: Taejin Hwang

\qns{SVD Intuition}

In this question, we will investigate into some of the intuition behind the "Swiss-Army Knife" of Linear Algebra, the \textbf{Singular Value Decomposition.} You may have seen the following forms of the SVD of an $m \times n$ matrix $A$ from lecture, homework, and discussion:
\begin{equation}
  A = U \Sigma V^{T} = \sum\limits_{i} \sigma_{i} \vec{u}_{i} \vec{v}_{i}^T
\end{equation}
Where $\sigma_{i}$ are referred to as the \textbf{singular values} and $\vec{u}_{i}, \vec{v}_{i}$ are the $i^{th}$ column of $U$ and $V$ respectively.

The Spectral Theorem which says that any \textbf{symmetric} matrix is orthogonally diagonalizable, will play a huge part in this questoin. 
This means that a symmetric matrix will always have $n$ linearly independent eigenvectors that are all mutually orthogonal.

We will start our investigation by looking at the eigenvectors of the matrix $A^{T} A.$

\begin{enumerate}[resume]
  \qitem Let $\vec{v}$ be a nonzero vector in $\mathbb{R}^m$ that is also in $\text{Col}(A).$ 
  Show that if $\vec{v}$ is an eigenvector of $A^{T} A,$ with eigenvalue $\lambda,$ there must also be an eigenvector $\vec{u}$ of $AA^{T}$ with eigenvalue $\lambda.$

  \sol {
    Since $\vec{v}$ is an eigenvector of $A^{T} A,$ we know that $A^{T}A \vec{v} = \lambda \vec{v}.$ \vskip 1pt
    If we left multiply by $A$ we see that 
    \begin{equation}
      A A^{T}A \vec{v} = A (\lambda \vec{v}) = \lambda A \vec{v} 
    \end{equation}
    Since $A \vec{v} \in \text{Col}(A),$ it must be nonzero. 
    Therefore, we can say that $\vec{u} = A \vec{v}$ is an eigenvector of $AA^{T}$ with eigenvalue $\lambda.$
  }

  \qitem Let $\vec{u}$ be a nonzero vector in $\mathbb{R}^n$ that is also in $\text{Row}(A).$
  Show that if $\vec{u}$ is an eigenvector of $AA^{T},$ with eigenvalue $\lambda,$ there must also be an eigenvector $\vec{v}$ of $A^{T} A$ with eigenvalue $\lambda.$

  \sol {
    We proceed in a similar manner: Since $\vec{v}$ is an eigenvector of $AA^{T},$ we know that $AA^{T} \vec{u} = \lambda \vec{u}.$ \vskip 1pt
    If we left multiply by $A^{T}$ we see that 
    \begin{equation}
      A^{T}AA^{T} \vec{u} = A^{T} (\lambda \vec{u}) = \lambda A^{T} \vec{u} 
    \end{equation}
    Since $A \vec{u} \in \text{Row}(A),$ it must be nonzero. 
    Therefore, we can say that $\vec{v} = A^{T} \vec{u}$ is an eigenvector of $A^{T}A$ with eigenvalue $\lambda.$
  }

  \qitem We can pick $\vec{v}$ to be an eigenvector of $A^{T}A$ with unit norm. What would the norm of $A \vec{v},$ be?

  \sol {
    The $l^{2}$ norm of a vector is the square root of its inner product with itself:
    \begin{equation} 
      \norm{A \vec{v}}_{2} = \sqrt{(A \vec{v})^{T} (A \vec{v})} = \sqrt{\vec{v}^{T} A^{T} A \vec{v}} = \sqrt{\vec{v}^{T} \lambda \vec{v}} = \sqrt{\lambda \vec{v}^{T} \vec{v}} = \sqrt{\lambda \norm{v}_2} = \sqrt{\lambda}
    \end{equation}
    Note that $\norm{A \vec{v}}_{2}$ cannot be imaginary since we already showed that $\lambda \geq 0$ in part (b).
  }

  \qitem Going back to part (d), let's normalize $\vec{u}$ and $\vec{v}$ to have unit norm.
  Can you give a relationship between $\vec{u}$ and $\vec{v}$ using the fact that they have unit norm?

  \sol {
    We showed in part (e) that if $\vec{v}$ had unit norm, $\norm{A \vec{v}} = \sqrt{\lambda}.$ \vskip 1pt
    In part (d) we showed that $A \vec{v}$ is an eigenvector of $A A^{T}.$ 
    Therefore, we can say that $A \vec{v} = \sigma \vec{u}$ for some scalar $\sigma.$ 
    Taking the norm of both sides, we will see that:
    \begin{equation}
      \norm{A \vec{v}} = \norm{\sigma \vec{u}} = \sigma \norm{\vec{u}} = \sigma
    \end{equation}
    This shows that $\sigma = \sqrt{\lambda}$ so we see that $A \vec{v} = \sqrt{\lambda} \vec{u}.$
  }

  \qitem Now let $\vec{v}$ be nonzero vector in $\mathbb{R}^{n}$ that is also in the null space of $A.$
  Show that $\vec{v}$ is an eigenvector of $A^{T} A$ with eigenvalue $0.$

  \sol {
    If $\vec{v} \in \text{Nul}(A),$ then $A \vec{v} = \vec{0}.$ Left multiplying, by $A^{T}$ we see that:
    \begin{equation}
      A^{T} (A \vec{v}) = A^{T} \vec{0} = \vec{0} = 0 \cdot \vec{v}
    \end{equation}
    Therefore, $\vec{v}$ is an eigenvector of $A^{T} A$ with eigenvalue $0.$
  }

  \qitem Similarly, let $\vec{u}$ be nonzero vector in $\mathbb{R}^{m}$ that is also in the null space of $A^{T}.$
  Show that $\vec{u}$ is an eigenvector of $AA^{T}$ with eigenvalue $0.$

  \sol {
    If $\vec{u} \in \text{Nul}(A^{T}),$ then $A^{T} \vec{u} = \vec{0}.$ Left multiplying, by $A$ we see that:
    \begin{equation}
      A (A^{T} \vec{u}) = A \vec{0} = \vec{0} = 0 \cdot \vec{u}
    \end{equation}
    Therefore, $\vec{u}$ is an eigenvector of $A A^{T}$ with eigenvalue $0.$
  }

  \qitem We will finally bring everything together using all of the facts proven above.
  If $A$ is a $m \times n$ matrix, show that $AV = U \Sigma.$ \textit{Hint: Try writing out the relationship between $\vec{v}_{i}$ and $\vec{u}_{i}$ and try turning these vector equations into a matrix equation.}

  \sol {
    We will consider the cases for when $m \geq n$ and $m < n.$ \\
    Suppose $m \geq n.$ \\
    We've shown for eigenvectors of $A^{T} A, \ \vec{v}_{1}, \cdots, \vec{v_k} \in \text{Col}(A), \ A \vec{v}_{1} = \sigma_{1} \vec{u}_{1}, \cdots, A \vec{v_k} = \vec{u_k}.$ \\
    Therefore, if we try turning this into a matrix equation, we see that:
    $$A \begin{bmatrix} | & \ & | \\
    \vec{v}_{1} & \cdots & \vec{v}_{k}  \\ | & \ & | \end{bmatrix} 
    =  \begin{bmatrix} | & \ & | \\ \sigma_{1} \vec{u}_{1} & \cdots & \sigma_{k} \vec{u}_{k} \\ | & \ & | \end{bmatrix}$$
    For the remaining eigenvectors of $A^{T}A, \vec{v}_{k + 1}, \cdots, \vec{v}_{n} \in \text{Nul}(A), \ A \vec{v_{i}} = 0 \vec{u_{i}},$ up until $n.$ Therefore, we can try extending our matrix to all of the columns of $V.$\\
    $$A \begin{bmatrix} | & \ & | \\
    \vec{v}_{1} & \cdots & \vec{v}_{n}  \\ | & \ & | \end{bmatrix} 
    =  \begin{bmatrix} | & \ & | \\ \sigma_{1} \vec{u}_{1} & \cdots & \sigma_{k} \vec{u}_{n} \\ | & \ & | \end{bmatrix} = 
     \begin{bmatrix} | & \ & | \\ \vec{u}_{1} & \cdots & \vec{u}_{n} \\ | & \ & | \end{bmatrix} \begin{bmatrix} \sigma_{1} & 0 &  \cdots & 0 \\ 0 & \sigma_{2} & \cdots & 0 \\ \vdots & \vdots & \ddots & \sigma_{n} \\ 
    \vdots & \vdots & \ddots & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix} = U \Sigma$$
    Now let's $m < n.$ \\
    We start off wit the eigenvectors of $A^{T} A$ again: $A \vec{v}_{1} = \sigma_{1} \vec{u}_{1}, \cdots, A \vec{v_k} = \vec{u_k}.$ \\
    We will get the same matrix equation:
    $$A \begin{bmatrix} | & \ & | \\
    \vec{v}_{1} & \cdots & \vec{v}_{k}  \\ | & \ & | \end{bmatrix} 
    =  \begin{bmatrix} | & \ & | \\ \sigma_{1} \vec{u}_{1} & \cdots & \sigma_{k} \vec{u}_{k} \\ | & \ & | \end{bmatrix}$$
    For the remaining eigenvectors of $A^{T}A, \vec{v}_{k + 1}, \cdots, \vec{v}_{n} \in \text{Nul}(A), \ A \vec{v_{i}} = 0 \vec{u_{i}},$ up until $m < n.$ Notice that we cannot cover all eigenvectors of $A^{T} A$ but we will cover up to $m < n.$
    $$A \begin{bmatrix} | & \ & | \\
    \vec{v}_{1} & \cdots & \vec{v}_{m}  \\ | & \ & | \end{bmatrix} = 
    \begin{bmatrix} | & \ & | \\ \sigma_{1} \vec{u}_{1} & \cdots & \sigma_{m} \vec{u}_{m} \\ | & \ & | \end{bmatrix} = 
     \begin{bmatrix} | & \ & | \\ \vec{u}_{1} & \cdots & \vec{u}_{m} \\ | & \ & | \end{bmatrix} \begin{bmatrix} \sigma_{1} & 0 &  \cdots & 0 \\ 0 & \sigma_{2} & \cdots & 0 \\ \vdots & \vdots & \ddots & \sigma_{m} \end{bmatrix}$$
    We however, can still extend to $V$ since $A \vec{v}_{i} = \vec{0}$ for $i \geq n.$ We will do this by padding $0$'s to the right of the $\Sigma$ matrix.
    $$A \begin{bmatrix} | & \ & | \\
    \vec{v}_{1} & \cdots & \vec{v}_{n}  \\ | & \ & | \end{bmatrix} = 
    \begin{bmatrix} | & \ & | &  & | \\ \sigma_{1} \vec{u}_{1} & \cdots & \sigma_{m} \vec{u}_{m} & \cdots & \vec{0} \\ | & \ & | &  & | \end{bmatrix} = 
     \begin{bmatrix} | & \ & | \\ \vec{u}_{1} & \cdots & \vec{u}_{m} \\ | & \ & | \end{bmatrix} \begin{bmatrix} \sigma_{1} & 0 &  \cdots & 0 \\ 0 & \sigma_{2} & \cdots & 0 & \cdots 0 \\ \vdots & \vdots & \ddots & \sigma_{m} & \cdots 0 \end{bmatrix}$$
    In both cases, we see that $AV = U\Sigma .$
  }

  \qitem Why does the above equation imply that $A = U \Sigma V^{T}?$

  \sol {
    Since $V$ is an orthonormal matrix, we can right multiply by $V^{-1} = V^{T}$ to get:
    \begin{equation}
      A = U \Sigma V^{T}
    \end{equation}
  }

\end{enumerate}