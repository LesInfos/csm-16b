% Author: Taejin Hwang

\qns{Moore-Penrose Pseudoinverse}

\meta {
  If students ask about proving the minimality of the Moore-Penrose Pseudo Inverse, refer back to the last part of the SVD Intuition question.
  The minimality is connected to the $\text{Col}(A^{T})$ and $\text{Nul}(A)$ and picking "weights" that are solely in $\text{Col}(A^{T})$ and making all of the weights for $\text{Nul}(A)$ zero.
}

We've seen many different ways to solve the matrix-vector equation:
\begin{equation}
  A \vec{x} = \vec{b}
\end{equation}
We could use Gaussian-Elimination to systematically solve for $\vec{x},$ but we came up with better techniques each depending on the shape of an $m \times n$ matrix $A.$
\begin{itemize}
  \item If $A$ was a square, invertible, matrix, we saw that $\vec{x} = A^{-1} \vec{b}$
  \item If $A$ was full rank, and $m > n,$ we could use Least Squares to say that $\hat{x} = (A^{T} A)^{-1} A^{T} \vec{b}.$
  \item If $A$ had more columns than rows, or $n > m$ and the solution was "sparse," then we could use OMP to solve for $\vec{x}.$
\end{itemize}
We will now come up with an alternate solution for when $n > m$ called the \textbf{Moore-Penrose} pseudoinverse.

\meta {
  Remember that there is no unique solution to this system of equations, and there will be infinitely many solutions. 
  However, we use the Moore-Penrose pseudoinverse since it has minimum norm, or minimum energy in a physical context.
}
We denote the pseudoinverse with a dagger, and the solution will be $\vec{x} = A^{\dagger} \vec{b}.$
This solution is special in that it has \textbf{minimum norm.} That is, if we have another solution $\vec{z},$ such that $A \vec{z} = \vec{b},$ then $\norm{x} \leq \norm{z}.$

\begin{enumerate}
  \qitem Let's start with the original equation $A \vec{x} = \vec{b}.$ What is the formula for the SVD of $A?$

  \sol {
    The SVD of $A$ is $A = U \Sigma V^{T}.$
  }

  \qitem We will first assume that $A$ is full row rank or $\text{Rank}(A) = m$ and try undoing the SVD. What will the $\Sigma$ matrix look like?

  \sol {
    Since $A$ is an $m \times n$ matrix, $\Sigma$ will be also be $m \times n.$
    We assumed that $n > m,$ and that $A$ is full rank, so $\Sigma$ will be a matrix with more columns than rows but will have a nonzero entry $\sigma_{i}$ in the $i^{th}$ row and column.
    $$\Sigma = \begin{bmatrix} \sigma_{1} & 0 & \cdots & 0 & \cdots & 0 \\ 0 & \sigma_{2} & \cdots & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \sigma_{m} & \cdots&  0 \end{bmatrix}$$
  }

  \qitem Note that this matrix $\Sigma$ does not have an inverse since it isn't square. We can not perfectly undo the effects of $\Sigma$ however, we can construct a matrix $\widetilde{\Sigma}$ such that $\widetilde{\Sigma} \Sigma$ will be an $n \times n$ matrix that is the identity, $I_{m},$ with zeros padded below and to the right.
  What is this matrix $\widetilde{\Sigma}?$

  \meta {
    The key to this problem as a whole, is that we cannot perfectly recover $\vec{x}$ since our system was underdetermined to begin with. The best we can do is approximate it using the information we have (the $m$ singular values of $A.$)
  }

  \sol {
    The matrix $\Sigma$ is almost diagonal. It has $0$ entries to the right of the $m^{th}$ column.
    Also, since $A$ is full rank, all of the entries on the diagonal are nonzero. 
    It follows that the following matrix will make $\widetilde{\Sigma} \Sigma = I_{m}$ with zeros padded on the right and below.
    $$\widetilde{\Sigma} = \begin{bmatrix} \frac{1}{\sigma_{1}} & 0 &  \cdots & 0 \\ 0 & \frac{1}{\sigma_{2}} & \cdots & 0 \\ \vdots & \vdots & \ddots & \frac{1}{\sigma_{m}} \\ 
    \vdots & \vdots & \ddots & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix}$$

    This can also be realized by looking at the matrix equation $\Sigma \vec{w} = \vec{z}$ where $\vec{z}$ is a vector in $\mathbb{R}^{n}$ and $\vec{w}$ is a vector in $\mathbb{R}^{m}.$ 
    We can uncouple these equations to write:
    $$\sigma_{1} w_{1} = z_{1}, \ \dotsc, \  \sigma_{m} w_{m} = z_{m}.$$
    The $w_{i}$ for $i = 1,\ldots,m$ can be solved accordingly:
    $$w_{1} = \frac{z_{1}}{\sigma_{1}}, \ \dotsc, \ w_{m} = \frac{z_{m}}{\sigma_{m}}$$
    We cannot solve for $z_{i}$ for $i > m$ since the singular values $\sigma_{i} = 0.$ 
    Therefore, the best we can do is approximate $z_{i}$ as $0$ to conclude that the $\widetilde{\Sigma}$ matrix is
    $$\widetilde{\Sigma} = \begin{bmatrix} \frac{1}{\sigma_{1}} & 0 &  \cdots & 0 \\ 0 & \frac{1}{\sigma_{2}} & \cdots & 0 \\ \vdots & \vdots & \ddots & \frac{1}{\sigma_{m}} \\ 
    \vdots & \vdots & \ddots & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix}$$

  }

  \qitem How can we find $U^{-1}$ and $V^{-1}$ using what we know about matrices $U$ and $V?$

  \sol {
    The $U$ and $V$ matrices have orthonormal columns, meaning $U^{T} U = I_{m}$ and $V^{T} V = I
    _{n}$ \\
    It follows from this fact that $U^{-1} = U^{T}$ and $V^{-1} = V^{T}.$ 
  } 

  \qitem Use the special facts in the previous part about the $U, V,$ and $\Sigma$ matrices to "undo" the SVD.

  \sol {
    We first write out the original equation combined with the SVD:
    $$U \Sigma V^{T} \vec{x} = \vec{b}$$
    Then we can left multiply by $U^{T}$
    $$\Sigma V^{T} \vec{x} = U^{T} \vec{b}$$
    Since $\widetilde{\Sigma} \Sigma = I,$ we can use this matrix to undo the effect of $\Sigma.$
    $$V^{T} \vec{x} = \widetilde{\Sigma} U^{T} \vec{b}$$
    Lastly, we left multiply by $V$ to get
    $$\vec{x} = V \widetilde{\Sigma} U^{T} \vec{b}.$$
    This matrix $V \widetilde{\Sigma}U^{T}$ is referred to as the pseudoinverse.
    $$A^{\dagger} = V \widetilde{\Sigma}U^{T}$$
  }

  \qitem We made the assumption that $A$ was a full rank matrix, but this will not always be the case. Therefore, in the case in which $A$ is of rank $k < m,$ we will use the \textbf{compact} SVD. 
  This was derived in question 2 (g), and can be expressed in the following form:
  \begin{equation}
    A = \sum\limits_{i=1}^{k} \sigma_{i} \vec{u}_{i} \vec{v}_{i}^{T} = U_{k} \Sigma_{k} V_{k}^{T}
  \end{equation}
  In this condensed form, $U_{k}$ is a $m \times k$ matrix, $\Sigma_{k}$ is a $k \times k$ matrix, and $V_{k}^{T}$ is a $k \times n$ matrix.

  \textbf{What is the solution $\vec{x}$ in this case?}

  \meta {
    This uses the compact SVD derived in the SVD intution question.
  }

  \sol {
    The exact same procedures can be taken as in the previous part. We can do this since $U_{k}^{T} U_{k} = I_{m}$ and $V_{k}^{T}V_{k} = I_{n}.$ In addition, we can notice that $\Sigma_{k}$ is invertible since it is a diagonal matrix with no zero entries on the diagonal. Therefore the solution will be:
    $$\vec{x} = A^{\dagger} \vec{b} = V_{k} \Sigma_{k}^{-1} U_{k}^{T}$$
  }
\end{enumerate}