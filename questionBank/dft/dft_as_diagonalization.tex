%author Alexander Feng
% This question arises from a natural question: DFT for diagonalization?
% Credit to Gilbert Strang from MIT for setting the basis to write this question.

\qns{DFT as a basis for Diagonalization}

Looking at a DFT matrix, notice how the DFT matrix's columns could form a very nice basis.
In fact, a natural question to ask is what types of matrices can be diagonalized by the DFT?
Let's start with a motivating example.

\begin{enumerate}

\qitem
What's the following matrix's eigenvectors?

{\em HINT: Don't use the determinant. Try guessing and noticing the special properties of the matrix.}
\begin{align*}
  A =
\begin{bmatrix}
1 & 4 & 3 & 2 \\
2 & 1 & 4 & 3 \\
3 & 2 & 1 & 4 \\
4 & 3 & 2 & 1 \\
\end{bmatrix}
\end{align*}

\sol{
The most obvious guess is try a vector of only ones because each row adds up to 10.
This has eigenvalue 10.
Afterwards, we can leverage the cyclic nature of the rows, and find another eigenvector in
$\begin{bmatrix}
1\\
-1\\
1\\
-1\\
\end{bmatrix}$
This vector has eigenvalue -2.
Taking a very wild guess, well try using gram schmidt to figure out the next eigenvectors.
Perhaps we'll be lucky.
What you'll find is that the other two eigenvectors are in fact the other two basis vectors for a size 4 DFT matrix!
Thus, the eigenvectors (unnormalized) are:
\begin{align*}
  \{
\begin{bmatrix}
1\\
1\\
1\\
1\\
\end{bmatrix},
\begin{bmatrix}
1\\
\omega\\
\omega^{2}\\
\omega^{3}\\
\end{bmatrix},
\begin{bmatrix}
1\\
\omega^{3}\\
\omega^{6}\\
\omega^{9}\\
\end{bmatrix},
\begin{bmatrix}
1\\
\omega^{4}\\
\omega^{8}\\
\omega^{12}\\
\end{bmatrix}
\}
\end{align*}

Note that in gram schmidt, you would apply a normalization factor to each DFT basis vector.
}

\qitem
This seems quite interesting.
Let's try generalizing the matrix from part (a).
We define a permutation matrix as a matrix who has exactly one entry of 1 in each row.
You can think of this as an identity matrix with all of its rows scrambled about.
Write the matrix $A$ from part (a) as a sum of permutation matrices.

\sol{
What we find as our solution is the following:
\begin{align*}
A = 1I + 2
\begin{bmatrix}
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
\end{bmatrix}
+ 3
\begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
\end{bmatrix}
+ 4
\begin{bmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 \\
\end{bmatrix}
\end{align*}
}

\qitem
From part (b), you should notice that there's an interesting cyclic rotation of rows in the permutation matrices.
We define the permutation matrix $P$ as follows:

Write all the other permutation matrices from part (b) in terms of $P$.
Then, rewrite all the matrix $A$ in terms of permutation matrices.

\sol{
You should find that if a given permutation matrix $P_{n}$ has $n$ cyclic row shifts away from the identity, then it can be written as $P_{n} = P^{n}$.

For example, if we rewrote the solution to part (b) as $A = 1I + 2P_{1} + 3P_{2} + 4P_{3}$.

We could rewrite it in terms of $P$ as $A = 1I + 2P^{1} + 3P^{2} + 4P^{3}$.

You can verify this solution for yourself.
}

\qitem
For the 4x4 case, notice that $P^{4} = I$.
For the NxN case, it's simple to also prove that $P^{N} = I$.
This stems from how $P$ multiply $P$ produces one circular row shift.
Very quickly, we can see the nature of the permutation matrices as eerily similar to the roots of unity.
Perhaps we can discern the relationship between a permutation matrix, its eigenvectors, and its eigenvalues.
Given the permutation matrix $P$ as defined before.
Suppose that we have the eigenvalue $\lambda$ and the vector
\begin{align*}
\vec{x} =
\begin{bmatrix}
x_{1}\\
x_{2}\\
x_{3}\\
x_{4}
\end{bmatrix}
\end{align*}
What's the relationship between each of the elements of $\vec{x}$ \hspace{0.2mm}?
\end{enumerate}

\sol{
Multiplying everything out, we observe that
\begin{align*}
P\vec{x} =
\lambda
\begin{bmatrix}
x_{2}\\
x_{3}\\
x_{4}\\
x_{5}
\end{bmatrix} \implies \\
x_{1} = \lambda x_{2} \\
x_{2} = \lambda x_{3} \\
x_{3} = \lambda x_{4} \\
x_{4} = \lambda x_{1} \\
\end{align*}
}
