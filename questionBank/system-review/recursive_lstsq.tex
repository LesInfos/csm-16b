% Author: Alexander Feng
% Email: alexfeng2000@berkeley.edu
% Inspiration: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-241j-dynamic-systems-and-control-spring-2011/readings/MIT6_241JS11_chap02.pdf

\qns{Recursive Least Squares}

In regression style problems, the general goal is to find a set of solutions that minimize an error.
In the case of System ID, we have a system $A\vec{x} = \vec{b}$ and want to minimize the magnitude of the error vector.
Recall that $A$ is a matrix of inputs and $\vec{b}$ is the vector of observed outputs.
$\vec{x}$ is a vector of unknown parameters and $\vec{e} = A\vec{x} - \vec{b}$ is our error vector.

The solution $\vec{x}$ that minimizes $||\vec{e}||$ is
\begin{align*}
\vec{x} = (A^{T}A)^{-1}A^{T}\vec{b}
\end{align*}

\begin{enumerate}

\qitem
Suppose that we're performing System ID and have already computed $\vec{x}$. We suddenly get new data points, should we continue using $\vec{x}$? Why or why not? If we don't continue using $\vec{x}$, what should we do?

\sol{
You don't want to continue using the old $\vec{x}$ because new data can change your solution and what $\vec{x}$ minimizes the error across \emph{all} your data.
The only exception to this is if you had perfect data for all old data and new data.
Thus, we should perform an update to our solution $\vec{x}$.
}

\qitem
Armed with our new data, why might it be a bad idea to recompute our entire least squares solution?

\sol{
Inverting a matrix is very costly and quite difficult.
You can imagine if we had thousands of data points and only received a few hundred new points, inverting $(A^{T}A)$ would be a nightmare.
}

\qitem
To simplify things, let's think about updating our least squares solution with just one data point.
Suppose that we have computed a vector $\vec{x_k}$ for $k$ data points.
We received a new data point ($a_{k+1}$, $b_{k+1}$) and would like to compute $\vec{x_{k+1}}$.
Formulate this as a new least squares problem.

\sol{
\begin{align*}
\begin{bmatrix}
\vec{b_k} \\
b_{k+1}
\end{bmatrix} =
\begin{bmatrix}
A_{k} \\
a_{k+1}
\end{bmatrix}
\vec{x_{k + 1}} +
\begin{bmatrix}
\vec{e_k}\\
e_{k+1}
\end{bmatrix}
\end{align*}
}

\qitem
Starting off with $A^{T}A\vec{x} = A^{T}\vec{b}$, rewrite everything in terms of summations of the $A$ matrix's rows and columns.

\sol{
\begin{align*}
A^{T}A\vec{x} = A^{T}\vec{b} = \\
\left(\sum_{i = 1}^{k} A^{T}[i]A\{i\}\right)\vec{x} = \sum^{k}_{i=1}A^{T}[i]b_i
\end{align*}
where $[i]$ is indicing into the column and $\{i\}$ is the row.
}

\qitem
If we define $Q_k = A^{T}A$, write out $\vec{x}_{k+1}$ in terms of $\vec{x}_{k}$, our previously computed solution, ($a_{k+1}$, $b_{k+1}$), $Q_k$, summations, and other known values.

\sol{
We get the following after substituting where $A$ is a matrix of our inputs, including our new data.
\begin{align*}
Q_{k+1} = A^{T}A = \left(\sum_{i = 1}^{k+1} A^{T}[i]A\{i\}\right) \\
\vec{x}_{k+1} = Q^{-1}_{k+1}\left[Q_{k}\vec{x}_{k} + A^{T}[k+1]b_{k+1}\right] \\
\end{align*}
Notice that we now have a recursive way to update our least squares solution.

Substituting $Q_{k+1} = Q_{k} + A^{T}[k+1]A\{k+1\}$, we get:
\begin{align*}
\vec{x}_{k+1} = \vec{x}_{k} + Q^{-1}_{k+1}A^{T}[k+1]\left(b_{k+1} - A\{k+1\}\vec{x}_{k}\right)
\end{align*}
}

\qitem
Why is this recursive solution actually not much of an improvement over just recomputing the entire least squares solution?

\sol{
It's not much of an improvement because we still have to invert $A^{T}A$!
As it turns out however, there's also a recursive way to compute the inverse of $Q_{k+1}$ given $Q_{k}$, but we won't talk about it here.
Using the recursive inversion technique results in a order of magnitude speed up over recomputing the entire least squares solution.
}


\end{enumerate}
