% Authors: Taejin Hwang, Justin Yu
% Emails: taejin@berkeley.edu, justinvyu@berkeley.edu

\qns{Inner Products and Orthogonality}

Recall from EECS 16A that we defined the notion of multiplying two vectors by using an \textbf{inner product}.
An inner product over the real numbers, is a rule that takes two vectors $\vec{x}, \vec{y} \in V$ and outputs a scalar in $\mathbb{R}$ that satisfies the following properties:

\meta {
  Note that the properties below are for $\textbf{real}$ inner products, and \textbf{complex} inner products will be covered later on when doing the DFT.
}

\begin{enumerate}[label=(\roman*)]
  \item Symmetry: $\innp{\vec{x}}{\vec{y}} = \innp{\vec{y}}{\vec{x}}$
  \item Homogeneity: $\innp{\alpha \vec{x}}{\vec{y}} = \alpha \innp{\vec{x}}{\vec{y}}$
  \item Additivity: $\innp{\vec{x}+\vec{y}}{\vec{z}} = \innp{\vec{x}}{\vec{z}} + \innp{\vec{y}}{\vec{z}}$
  \item Positive-definiteness: $\innp{\vec{x}}{\vec{x}} \geq 0$, and is $= 0$ iff $\vec{x} = \vec{0}$
\end{enumerate}


In this question, we will review the concepts of an inner product, and focus our attention on when two vectors are \textbf{orthogonal}. We say two vectors $\vec{x}$ and $\vec{y}$ are orthogonal if:
\begin{equation}
  \langle \vec{x}, \vec{y} \rangle = 0
\end{equation}

If we plotted these vectors geometrically, then the two vectors would appear to be perpendicular.
\begin{enumerate}
  \qitem What is the difference between a dot product and an inner product?
  
  \meta {
    The purpose of this part is to make sure students understand that not all inner products are the dot product, since they will be seeing the Frobenius Inner Product later on in the course. 
  }

  \sol {
    A dot product is a special case of the inner product in $\mathbb{R}^{n}$ where:
    \begin{equation}
      \innp{\vec{x}}{\vec{y}} = \vec{x}^{T} \vec{y}
    \end{equation}
    An inner product is a generalization in which we multiply two vectors in a vector space that satisfies the four properties above.
    We will take a look at other inner products that are not necessarily the dot product, later on in the course.
  }

  \qitem We can define the \textbf{norm} of a vector using an inner product. 
  We define the \textbf{Euclidean,} norm as:
  \begin{equation}
    \norm{\vec{x}} = \sqrt{\innp{\vec{x}}{\vec{x}}}
  \end{equation}
  Recall the cosine definition of a dot product using norms, where $\theta$ is the angle between $\vec{x}$ and $\vec{y}$:
  \begin{equation}
    \vec{x} \cdot \vec{y} = \norm{\vec{x}} \norm{\vec{y}} \cos(\theta)
  \end{equation}
  Show that if $\vec{x}$ and $\vec{y}$ are non-zero and orthogonal, then the angle between the two vectors is $90^{\circ}.$

  \sol {
    If $\vec{x}$ and $\vec{y}$ are orthogonal, then their inner product must be zero.
    $$\innp{\vec{x}}{\vec{y}} = 0.$$
    This means that $\norm{\vec{x}} \norm{\vec{y}} \cos(\theta) = 0.$ Since $\vec{x}, \vec{y} \neq \vec{0},$ $\norm{\vec{x}}, \norm{\vec{y}}$ must be non-zero, which implies that $\cos(\theta) = 0.$
    We conclude by saying that $\theta = \frac{\pi}{2}$ or $90^{\circ}.$
  }

  \qitem Show that $\vec{0}$ is orthogonal to any vector $\vec{x} \in \mathbb{R}^{n}$ using the definition of the dot product.

  \sol {
    $$\vec{x} \cdot \vec{0} = \vec{x}^{T} \vec{0} = x_{1} \cdot 0 + \cdots x_{n} \cdot 0 = 0.$$
  }

  \qitem Show that if $\vec{v}_{1}$ and $\vec{v}_{2}$ are non-zero and orthogonal, then they must be linearly independent.

  \sol {
    We must show that if $\alpha_{1} \vec{v}_{1} + \alpha_{2} \vec{v}_{2} = \vec{0},$ then $\alpha_{1} = \alpha_{2} = 0.$ \vskip 1pt
    We start off by taking the inner product of both sides with $\vec{v}_{1}$
    $$\innp{\alpha_{1} \vec{v}_{1} + \alpha_{2} \vec{v}_{2}}{\vec{v}_{1}} = \innp{\vec{0}}{\vec{v}_{1}}$$
    Using the scaling and additivity properties of the inner product, we see that:
    $$\alpha_{1} \innp{\vec{v}_{1}}{\vec{v}_{1}} + \alpha_{2} \innp{\vec{v}_{2}}{\vec{v}_{1}} = \innp{\vec{0}}{\vec{v}_{1}}$$
    We know that $\vec{0}$ and $\vec{v}_{2}$ are both orthogonal to $\vec{v}_{1}$ so their inner products must be $0.$
    $$\alpha_{1} \innp{\vec{v}_{1}}{\vec{v}_{1}} = 0.$$
    We also know that $\vec{v}_{1}$ is non-zero so $\norm{\vec{v}_{1}} > 0.$
    Which implies that $\alpha_{1}$ must be zero. \vskip 1pt
    The exact same argument can be used one more time, by taking the inner product of both sides with $\vec{v}_{2}$ to show that $\alpha_{2}$ must also be zero, which shows that $\vec{v}_{1}$ and $\vec{v}_{2}$ must be linearly independent.
  }

  \qitem Generalize the argument above to show that if $\{\vec{v}_{1}, \cdots, \vec{v}_{n}\}$ is a set of non-zero orthogonal vectors, then the set must be linearly independent. 
  We say a set of vectors is orthogonal, if each pair of vectors is mutually orthogonal, that is $\innp{\vec{v}_{i}}{\vec{v}_{j}} = 0$ for any $i,j$ where $i \neq j.$

  \meta {
    If students aren't comfortable with summations, especially the $1 \leq i < j \leq n$ part, then use the first method provided in the solution.
  }

  \sol {
    The argument should almost be identical. We must show that if $\alpha_{1} \vec{v}_{1} + \cdots + \alpha_{n} \vec{v}_{n} = \vec{0},$ then $\alpha_{1} = \hdots = \alpha_{n} = 0.$ We can use the same argument by taking the inner product of both sides with $\vec{v}_{1}$
    to show that $\alpha_{1}$ is zero, and then generalize it for all $i = \{1, \hdots, n \}.$ \vskip 1pt
    However, another clever way to do this problem is to take the inner product of both sides with itself:
    $$\innp{\alpha_{1} \vec{v}_{1} + \cdots + \alpha_{n} \vec{v}_{n}}{\alpha_{1} \vec{v}_{1} + \cdots + \alpha_{n} \vec{v}_{n}} = \innp{\vec{0}}{\vec{0}}$$ 
    Again we use the distributive properties of additivity and scaling to see that:
    $$\sum\limits_{i = 1}^{n} \alpha_{i}^{2} \innp{\vec{v}_{i}}{\vec{v}_{i}} \ + \ \sum\limits_{1 \leq i < j \leq n} \alpha_{i} \alpha_{j} \innp{\vec{v}_{i}}{\vec{v}_{j}} \ + \ \sum\limits_{1 \leq j < i \leq n} \alpha_{i} \alpha_{j} \innp{\vec{v}_{j}}{\vec{v}_{i}} = \innp{\vec{0}}{\vec{0}}$$ 
    Since the vectors are all mutually orthogonal, we see that:
    $$\sum\limits_{i = 1}^{n} \alpha_{i}^{2} \innp{\vec{v}_{i}}{\vec{v}_{i}} = 0$$
    Since all of the vectors $\vec{v}_{i}$ are non-zero, $\norm{\vec{v}_{i}} > 0.$ In addition, all of the scalars, $\alpha_{i}^{2}$ are greater than or equal to zero.
    However, since the linear combination is still equal to $0,$ all of the $\alpha_{i}$ must be equal to $0$ meaning the set is linearly independent.
  }
\end{enumerate}


